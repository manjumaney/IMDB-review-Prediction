{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 3 – IMDB MOVIE REVIEW\n",
    "\n",
    "Context: IMDB dataset having 25K movie reviews for natural language processing or Text analytics.\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous \n",
    "benchmark datasets. We provide a set of 12,500 highly polar movie reviews for training and 12,500 for \n",
    "testing. Please use less data eg 6K reviews if you are facing memory issues but make sure to use equal \n",
    "number of positive and negative sentiment reviews. Mention clearly in the notebook, if you have used a \n",
    "reduced dataset.\n",
    "\n",
    "For more dataset information, please go through the following link,\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Dataset Source: given\n",
    "\n",
    "Task: Goal of this project is to predict the number of positive and negative reviews using classification.\n",
    "\n",
    "Implementation:\n",
    "- Preprocess Text Data(Remove punctuation, Perform Tokenization, Remove stopwords and \n",
    "Lemmatize/Stem)\n",
    "- Perform TFIDF Vectorization\n",
    "- Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting \n",
    "Classifier. Use Xgboost instead of Gradient Boosting if it's taking a very long time in \n",
    "GridSearchCV\n",
    "- Perform Final evaluation of models on the best parameter settings using the evaluation metrics\n",
    "- Report the best performing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>When I first tuned in on this morning news, I ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I got this one a few weeks ago and love it! It...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      I thought this was a wonderful way to spend ti...  positive\n",
       "1      Probably my all-time favorite movie, a story o...  positive\n",
       "2      I sure would like to see a resurrection of a u...  positive\n",
       "3      This show was an amazing, fresh & innovative i...  negative\n",
       "4      Encouraged by the positive comments about this...  negative\n",
       "...                                                  ...       ...\n",
       "24995  When I first tuned in on this morning news, I ...  negative\n",
       "24996  I got this one a few weeks ago and love it! It...  positive\n",
       "24997  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "24998  I am a Catholic taught in parochial elementary...  negative\n",
       "24999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing dataset\n",
    "df = pd.read_csv(\"C:/Users/Manju/Documents/Assignments/data set/IMDB_dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     25000 non-null  object\n",
      " 1   sentiment  25000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 390.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(102)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>When I first tuned in on this morning news, I ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>I got this one a few weeks ago and love it! It...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24898 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      I thought this was a wonderful way to spend ti...  positive\n",
       "1      Probably my all-time favorite movie, a story o...  positive\n",
       "2      I sure would like to see a resurrection of a u...  positive\n",
       "3      This show was an amazing, fresh & innovative i...  negative\n",
       "4      Encouraged by the positive comments about this...  negative\n",
       "...                                                  ...       ...\n",
       "24995  When I first tuned in on this morning news, I ...  negative\n",
       "24996  I got this one a few weeks ago and love it! It...  positive\n",
       "24997  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "24998  I am a Catholic taught in parochial elementary...  negative\n",
       "24999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[24898 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24893</th>\n",
       "      <td>When I first tuned in on this morning news, I ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24894</th>\n",
       "      <td>I got this one a few weeks ago and love it! It...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24895</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24896</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24897</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24898 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      I thought this was a wonderful way to spend ti...  positive\n",
       "1      Probably my all-time favorite movie, a story o...  positive\n",
       "2      I sure would like to see a resurrection of a u...  positive\n",
       "3      This show was an amazing, fresh & innovative i...  negative\n",
       "4      Encouraged by the positive comments about this...  negative\n",
       "...                                                  ...       ...\n",
       "24893  When I first tuned in on this morning news, I ...  negative\n",
       "24894  I got this one a few weeks ago and love it! It...  positive\n",
       "24895  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "24896  I am a Catholic taught in parochial elementary...  negative\n",
       "24897  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[24898 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.drop_duplicates()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 25000 rows and 2 columns\n",
      "Cleaned data has 24898 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data has {} rows and {} columns\".format(len(df), len(df.columns)))\n",
    "print(\"Cleaned data has {} rows and {} columns\".format(len(data), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This probably ranks in my Top-5 list of the fu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an exceptional film. It is part comedy...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Personally, I LOVED TRIS MOVIE! My best friend...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>How does a Scotsman in a kilt make love in the...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>Look at the all the positive user comments of ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>This is a very strange film, with a no-name ca...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>As far as Asian horror goes, I have seen my sh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>In an apparent attempt to avoid remaking the o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     This probably ranks in my Top-5 list of the fu...  positive\n",
       "1     I saw this film at the 2002 Toronto Internatio...  positive\n",
       "2     This is an exceptional film. It is part comedy...  positive\n",
       "3     Krajobraz po bitwie like many films of Wajda i...  positive\n",
       "4     Personally, I LOVED TRIS MOVIE! My best friend...  positive\n",
       "...                                                 ...       ...\n",
       "5995  How does a Scotsman in a kilt make love in the...  negative\n",
       "5996  Look at the all the positive user comments of ...  negative\n",
       "5997  This is a very strange film, with a no-name ca...  negative\n",
       "5998  As far as Asian horror goes, I have seen my sh...  negative\n",
       "5999  In an apparent attempt to avoid remaking the o...  negative\n",
       "\n",
       "[6000 rows x 2 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing text Data by removing punctuation, performing Tokenization, removing stopwords and Stemming\n",
    "\n",
    "positive_reviews = data[data['sentiment'] == 'positive']\n",
    "negative_reviews = data[data['sentiment'] == 'negative']\n",
    "\n",
    "num_samples = 3000\n",
    "\n",
    "positive_sampled = positive_reviews.sample(num_samples, random_state=42)\n",
    "negative_sampled = negative_reviews.sample(num_samples, random_state=42)\n",
    "\n",
    "data = pd.concat([positive_sampled, negative_sampled], ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    3000\n",
       "negative    3000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This probably ranks in my Top-5 list of the fu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This probably ranks in my Top5 list of the fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an exceptional film. It is part comedy...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This is an exceptional film It is part comedy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Personally, I LOVED TRIS MOVIE! My best friend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Personally I LOVED TRIS MOVIE My best friend t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  This probably ranks in my Top-5 list of the fu...  positive   \n",
       "1  I saw this film at the 2002 Toronto Internatio...  positive   \n",
       "2  This is an exceptional film. It is part comedy...  positive   \n",
       "3  Krajobraz po bitwie like many films of Wajda i...  positive   \n",
       "4  Personally, I LOVED TRIS MOVIE! My best friend...  positive   \n",
       "\n",
       "                                     body_text_clean  \n",
       "0  This probably ranks in my Top5 list of the fun...  \n",
       "1  I saw this film at the 2002 Toronto Internatio...  \n",
       "2  This is an exceptional film It is part comedy ...  \n",
       "3  Krajobraz po bitwie like many films of Wajda i...  \n",
       "4  Personally I LOVED TRIS MOVIE My best friend t...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data['body_text_clean'] = data['review'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This probably ranks in my Top-5 list of the fu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This probably ranks in my Top5 list of the fun...</td>\n",
       "      <td>[this, probably, ranks, in, my, top5, list, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>[i, saw, this, film, at, the, 2002, toronto, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an exceptional film. It is part comedy...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This is an exceptional film It is part comedy ...</td>\n",
       "      <td>[this, is, an, exceptional, film, it, is, part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>[krajobraz, po, bitwie, like, many, films, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Personally, I LOVED TRIS MOVIE! My best friend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Personally I LOVED TRIS MOVIE My best friend t...</td>\n",
       "      <td>[personally, i, loved, tris, movie, my, best, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  This probably ranks in my Top-5 list of the fu...  positive   \n",
       "1  I saw this film at the 2002 Toronto Internatio...  positive   \n",
       "2  This is an exceptional film. It is part comedy...  positive   \n",
       "3  Krajobraz po bitwie like many films of Wajda i...  positive   \n",
       "4  Personally, I LOVED TRIS MOVIE! My best friend...  positive   \n",
       "\n",
       "                                     body_text_clean  \\\n",
       "0  This probably ranks in my Top5 list of the fun...   \n",
       "1  I saw this film at the 2002 Toronto Internatio...   \n",
       "2  This is an exceptional film It is part comedy ...   \n",
       "3  Krajobraz po bitwie like many films of Wajda i...   \n",
       "4  Personally I LOVED TRIS MOVIE My best friend t...   \n",
       "\n",
       "                                 body_text_tokenized  \n",
       "0  [this, probably, ranks, in, my, top5, list, of...  \n",
       "1  [i, saw, this, film, at, the, 2002, toronto, i...  \n",
       "2  [this, is, an, exceptional, film, it, is, part...  \n",
       "3  [krajobraz, po, bitwie, like, many, films, of,...  \n",
       "4  [personally, i, loved, tris, movie, my, best, ...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nltk.corpus (from versions: none)\n",
      "ERROR: No matching distribution found for nltk.corpus\n"
     ]
    }
   ],
   "source": [
    "pip install nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Manju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This probably ranks in my Top-5 list of the fu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This probably ranks in my Top5 list of the fun...</td>\n",
       "      <td>[this, probably, ranks, in, my, top5, list, of...</td>\n",
       "      <td>[probably, ranks, top5, list, funniest, movies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>[i, saw, this, film, at, the, 2002, toronto, i...</td>\n",
       "      <td>[saw, film, 2002, toronto, international, film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an exceptional film. It is part comedy...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This is an exceptional film It is part comedy ...</td>\n",
       "      <td>[this, is, an, exceptional, film, it, is, part...</td>\n",
       "      <td>[exceptional, film, part, comedy, part, drama,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>[krajobraz, po, bitwie, like, many, films, of,...</td>\n",
       "      <td>[krajobraz, po, bitwie, like, many, films, waj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Personally, I LOVED TRIS MOVIE! My best friend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Personally I LOVED TRIS MOVIE My best friend t...</td>\n",
       "      <td>[personally, i, loved, tris, movie, my, best, ...</td>\n",
       "      <td>[personally, loved, tris, movie, best, friend,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  This probably ranks in my Top-5 list of the fu...  positive   \n",
       "1  I saw this film at the 2002 Toronto Internatio...  positive   \n",
       "2  This is an exceptional film. It is part comedy...  positive   \n",
       "3  Krajobraz po bitwie like many films of Wajda i...  positive   \n",
       "4  Personally, I LOVED TRIS MOVIE! My best friend...  positive   \n",
       "\n",
       "                                     body_text_clean  \\\n",
       "0  This probably ranks in my Top5 list of the fun...   \n",
       "1  I saw this film at the 2002 Toronto Internatio...   \n",
       "2  This is an exceptional film It is part comedy ...   \n",
       "3  Krajobraz po bitwie like many films of Wajda i...   \n",
       "4  Personally I LOVED TRIS MOVIE My best friend t...   \n",
       "\n",
       "                                 body_text_tokenized  \\\n",
       "0  [this, probably, ranks, in, my, top5, list, of...   \n",
       "1  [i, saw, this, film, at, the, 2002, toronto, i...   \n",
       "2  [this, is, an, exceptional, film, it, is, part...   \n",
       "3  [krajobraz, po, bitwie, like, many, films, of,...   \n",
       "4  [personally, i, loved, tris, movie, my, best, ...   \n",
       "\n",
       "                                    body_text_nostop  \n",
       "0  [probably, ranks, top5, list, funniest, movies...  \n",
       "1  [saw, film, 2002, toronto, international, film...  \n",
       "2  [exceptional, film, part, comedy, part, drama,...  \n",
       "3  [krajobraz, po, bitwie, like, many, films, waj...  \n",
       "4  [personally, loved, tris, movie, best, friend,...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This probably ranks in my Top-5 list of the fu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This probably ranks in my Top5 list of the fun...</td>\n",
       "      <td>[this, probably, ranks, in, my, top5, list, of...</td>\n",
       "      <td>[probably, ranks, top5, list, funniest, movies...</td>\n",
       "      <td>[probabl, rank, top5, list, funniest, movi, iv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I saw this film at the 2002 Toronto Internatio...</td>\n",
       "      <td>[i, saw, this, film, at, the, 2002, toronto, i...</td>\n",
       "      <td>[saw, film, 2002, toronto, international, film...</td>\n",
       "      <td>[saw, film, 2002, toronto, intern, film, festi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an exceptional film. It is part comedy...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This is an exceptional film It is part comedy ...</td>\n",
       "      <td>[this, is, an, exceptional, film, it, is, part...</td>\n",
       "      <td>[exceptional, film, part, comedy, part, drama,...</td>\n",
       "      <td>[except, film, part, comedi, part, drama, part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Krajobraz po bitwie like many films of Wajda i...</td>\n",
       "      <td>[krajobraz, po, bitwie, like, many, films, of,...</td>\n",
       "      <td>[krajobraz, po, bitwie, like, many, films, waj...</td>\n",
       "      <td>[krajobraz, po, bitwi, like, mani, film, wajda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Personally, I LOVED TRIS MOVIE! My best friend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Personally I LOVED TRIS MOVIE My best friend t...</td>\n",
       "      <td>[personally, i, loved, tris, movie, my, best, ...</td>\n",
       "      <td>[personally, loved, tris, movie, best, friend,...</td>\n",
       "      <td>[person, love, tri, movi, best, friend, told, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  This probably ranks in my Top-5 list of the fu...  positive   \n",
       "1  I saw this film at the 2002 Toronto Internatio...  positive   \n",
       "2  This is an exceptional film. It is part comedy...  positive   \n",
       "3  Krajobraz po bitwie like many films of Wajda i...  positive   \n",
       "4  Personally, I LOVED TRIS MOVIE! My best friend...  positive   \n",
       "\n",
       "                                     body_text_clean  \\\n",
       "0  This probably ranks in my Top5 list of the fun...   \n",
       "1  I saw this film at the 2002 Toronto Internatio...   \n",
       "2  This is an exceptional film It is part comedy ...   \n",
       "3  Krajobraz po bitwie like many films of Wajda i...   \n",
       "4  Personally I LOVED TRIS MOVIE My best friend t...   \n",
       "\n",
       "                                 body_text_tokenized  \\\n",
       "0  [this, probably, ranks, in, my, top5, list, of...   \n",
       "1  [i, saw, this, film, at, the, 2002, toronto, i...   \n",
       "2  [this, is, an, exceptional, film, it, is, part...   \n",
       "3  [krajobraz, po, bitwie, like, many, films, of,...   \n",
       "4  [personally, i, loved, tris, movie, my, best, ...   \n",
       "\n",
       "                                    body_text_nostop  \\\n",
       "0  [probably, ranks, top5, list, funniest, movies...   \n",
       "1  [saw, film, 2002, toronto, international, film...   \n",
       "2  [exceptional, film, part, comedy, part, drama,...   \n",
       "3  [krajobraz, po, bitwie, like, many, films, waj...   \n",
       "4  [personally, loved, tris, movie, best, friend,...   \n",
       "\n",
       "                                   body_text_stemmed  \n",
       "0  [probabl, rank, top5, list, funniest, movi, iv...  \n",
       "1  [saw, film, 2002, toronto, intern, film, festi...  \n",
       "2  [except, film, part, comedi, part, drama, part...  \n",
       "3  [krajobraz, po, bitwi, like, mani, film, wajda...  \n",
       "4  [person, love, tri, movi, best, friend, told, ...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stemmed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 41265)\n",
      "['' '0' '00' ... 'œoliverâ' 'œoompahpahâ' 'œyouâ']\n"
     ]
    }
   ],
   "source": [
    "# Performing TFIDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['review'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>0000000000001</th>\n",
       "      <th>00000110</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>0010</th>\n",
       "      <th>007</th>\n",
       "      <th>0079</th>\n",
       "      <th>...</th>\n",
       "      <th>ã¼bertrash</th>\n",
       "      <th>ãœberbab</th>\n",
       "      <th>œconsid</th>\n",
       "      <th>œfamilyâ</th>\n",
       "      <th>œfood</th>\n",
       "      <th>œgolden</th>\n",
       "      <th>œiâ</th>\n",
       "      <th>œoliverâ</th>\n",
       "      <th>œoompahpahâ</th>\n",
       "      <th>œyouâ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0   00  0000000000001  00000110  0001  001  0010  007  0079  \\\n",
       "0  0.000000  0.0  0.0            0.0       0.0   0.0  0.0   0.0  0.0   0.0   \n",
       "1  0.000000  0.0  0.0            0.0       0.0   0.0  0.0   0.0  0.0   0.0   \n",
       "2  0.000000  0.0  0.0            0.0       0.0   0.0  0.0   0.0  0.0   0.0   \n",
       "3  0.049522  0.0  0.0            0.0       0.0   0.0  0.0   0.0  0.0   0.0   \n",
       "4  0.000000  0.0  0.0            0.0       0.0   0.0  0.0   0.0  0.0   0.0   \n",
       "\n",
       "   ...  ã¼bertrash  ãœberbab  œconsid  œfamilyâ  œfood  œgolden  œiâ  \\\n",
       "0  ...         0.0       0.0      0.0       0.0    0.0      0.0  0.0   \n",
       "1  ...         0.0       0.0      0.0       0.0    0.0      0.0  0.0   \n",
       "2  ...         0.0       0.0      0.0       0.0    0.0      0.0  0.0   \n",
       "3  ...         0.0       0.0      0.0       0.0    0.0      0.0  0.0   \n",
       "4  ...         0.0       0.0      0.0       0.0    0.0      0.0  0.0   \n",
       "\n",
       "   œoliverâ  œoompahpahâ  œyouâ  \n",
       "0       0.0          0.0    0.0  \n",
       "1       0.0          0.0    0.0  \n",
       "2       0.0          0.0    0.0  \n",
       "3       0.0          0.0    0.0  \n",
       "4       0.0          0.0    0.0  \n",
       "\n",
       "[5 rows x 41265 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names_out()\n",
    "X_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring parameter settings using GridSearchCV on Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_df, data['sentiment'], test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 ---- Precision: 0.755 / Recall: 0.654 / Accuracy: 0.709\n",
      "Est: 10 / Depth: 20 ---- Precision: 0.766 / Recall: 0.686 / Accuracy: 0.728\n",
      "Est: 10 / Depth: 30 ---- Precision: 0.771 / Recall: 0.738 / Accuracy: 0.749\n",
      "Est: 10 / Depth: None ---- Precision: 0.714 / Recall: 0.818 / Accuracy: 0.734\n",
      "Est: 50 / Depth: 10 ---- Precision: 0.86 / Recall: 0.709 / Accuracy: 0.788\n",
      "Est: 50 / Depth: 20 ---- Precision: 0.848 / Recall: 0.768 / Accuracy: 0.808\n",
      "Est: 50 / Depth: 30 ---- Precision: 0.845 / Recall: 0.81 / Accuracy: 0.823\n",
      "Est: 50 / Depth: None ---- Precision: 0.823 / Recall: 0.818 / Accuracy: 0.813\n",
      "Est: 100 / Depth: 10 ---- Precision: 0.874 / Recall: 0.746 / Accuracy: 0.812\n",
      "Est: 100 / Depth: 20 ---- Precision: 0.867 / Recall: 0.781 / Accuracy: 0.823\n",
      "Est: 100 / Depth: 30 ---- Precision: 0.849 / Recall: 0.818 / Accuracy: 0.829\n",
      "Est: 100 / Depth: None ---- Precision: 0.842 / Recall: 0.838 / Accuracy: 0.834\n"
     ]
    }
   ],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='negative', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))\n",
    "    \n",
    "for n_est in [10, 50, 100]:\n",
    "    for depth in [10, 20, 30, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\manju\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xgboost) (2.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\manju\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from xgboost) (1.14.0)\n",
      "Downloading xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/124.9 MB 3.3 MB/s eta 0:00:38\n",
      "   ---------------------------------------- 1.0/124.9 MB 3.9 MB/s eta 0:00:33\n",
      "   ---------------------------------------- 1.0/124.9 MB 3.9 MB/s eta 0:00:33\n",
      "   ---------------------------------------- 1.3/124.9 MB 1.3 MB/s eta 0:01:33\n",
      "    --------------------------------------- 2.1/124.9 MB 1.9 MB/s eta 0:01:04\n",
      "   - -------------------------------------- 3.1/124.9 MB 2.5 MB/s eta 0:00:50\n",
      "   - -------------------------------------- 4.5/124.9 MB 3.0 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 5.8/124.9 MB 3.4 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 7.9/124.9 MB 4.1 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 9.2/124.9 MB 4.4 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 11.0/124.9 MB 4.7 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 12.6/124.9 MB 5.0 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 14.4/124.9 MB 5.3 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 16.3/124.9 MB 5.5 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 18.6/124.9 MB 5.9 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 21.0/124.9 MB 6.2 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 23.6/124.9 MB 6.6 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 26.2/124.9 MB 6.9 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 28.8/124.9 MB 7.2 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 30.9/124.9 MB 7.3 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 33.8/124.9 MB 7.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 36.7/124.9 MB 7.9 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 39.8/124.9 MB 8.2 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 43.0/124.9 MB 8.5 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 43.0/124.9 MB 8.5 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 45.1/124.9 MB 8.2 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 48.2/124.9 MB 8.4 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 51.6/124.9 MB 8.7 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 55.6/124.9 MB 9.0 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 59.2/124.9 MB 9.3 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 64.0/124.9 MB 9.6 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 68.4/124.9 MB 10.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 72.4/124.9 MB 10.3 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 77.6/124.9 MB 10.7 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 82.6/124.9 MB 11.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 87.6/124.9 MB 11.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 93.1/124.9 MB 11.8 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 99.4/124.9 MB 12.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 104.1/124.9 MB 12.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 109.6/124.9 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 114.8/124.9 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 121.1/124.9 MB 13.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 13.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 124.9/124.9 MB 13.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__sklearn_clone__', '__sklearn_is_fitted__', '__str__', '__subclasshook__', '__weakref__', '_build_request_for_signature', '_can_use_inplace_predict', '_check_feature_names', '_check_n_features', '_configure_fit', '_create_dmatrix', '_doc_link_module', '_doc_link_template', '_doc_link_url_param_generator', '_estimator_type', '_get_default_requests', '_get_doc_link', '_get_iteration_range', '_get_metadata_request', '_get_param_names', '_get_tags', '_get_type', '_load_model_attributes', '_more_tags', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_set_evaluation_result', '_validate_data', '_validate_params', 'apply', 'best_iteration', 'best_score', 'classes_', 'coef_', 'evals_result', 'feature_importances_', 'feature_names_in_', 'fit', 'get_booster', 'get_metadata_routing', 'get_num_boosting_rounds', 'get_params', 'get_xgb_params', 'intercept_', 'load_model', 'n_features_in_', 'predict', 'predict_proba', 'save_model', 'score', 'set_fit_request', 'set_params', 'set_predict_proba_request', 'set_predict_request', 'set_score_request']\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "### Exploring parameter settings using XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(dir(xgb.XGBClassifier))\n",
    "print(xgb.XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 50 / Depth: 3 / LR: 0.01 ---- Precision: 0.809 / Recall: 0.523 / Accuracy: 0.688\n",
      "Est: 50 / Depth: 3 / LR: 0.1 ---- Precision: 0.831 / Recall: 0.683 / Accuracy: 0.762\n",
      "Est: 50 / Depth: 3 / LR: 1 ---- Precision: 0.786 / Recall: 0.778 / Accuracy: 0.774\n",
      "Est: 50 / Depth: 7 / LR: 0.01 ---- Precision: 0.812 / Recall: 0.562 / Accuracy: 0.704\n",
      "Est: 50 / Depth: 7 / LR: 0.1 ---- Precision: 0.826 / Recall: 0.744 / Accuracy: 0.785\n",
      "Est: 50 / Depth: 7 / LR: 1 ---- Precision: 0.805 / Recall: 0.787 / Accuracy: 0.79\n",
      "Est: 50 / Depth: 11 / LR: 0.01 ---- Precision: 0.804 / Recall: 0.645 / Accuracy: 0.733\n",
      "Est: 50 / Depth: 11 / LR: 0.1 ---- Precision: 0.822 / Recall: 0.776 / Accuracy: 0.796\n",
      "Est: 50 / Depth: 11 / LR: 1 ---- Precision: 0.805 / Recall: 0.8 / Accuracy: 0.795\n",
      "Est: 50 / Depth: 15 / LR: 0.01 ---- Precision: 0.798 / Recall: 0.67 / Accuracy: 0.74\n",
      "Est: 50 / Depth: 15 / LR: 0.1 ---- Precision: 0.828 / Recall: 0.794 / Accuracy: 0.807\n",
      "Est: 50 / Depth: 15 / LR: 1 ---- Precision: 0.798 / Recall: 0.782 / Accuracy: 0.783\n",
      "Est: 100 / Depth: 3 / LR: 0.01 ---- Precision: 0.82 / Recall: 0.525 / Accuracy: 0.692\n",
      "Est: 100 / Depth: 3 / LR: 0.1 ---- Precision: 0.842 / Recall: 0.736 / Accuracy: 0.791\n",
      "Est: 100 / Depth: 3 / LR: 1 ---- Precision: 0.811 / Recall: 0.803 / Accuracy: 0.8\n",
      "Est: 100 / Depth: 7 / LR: 0.01 ---- Precision: 0.812 / Recall: 0.616 / Accuracy: 0.726\n",
      "Est: 100 / Depth: 7 / LR: 0.1 ---- Precision: 0.832 / Recall: 0.784 / Accuracy: 0.805\n",
      "Est: 100 / Depth: 7 / LR: 1 ---- Precision: 0.815 / Recall: 0.797 / Accuracy: 0.8\n",
      "Est: 100 / Depth: 11 / LR: 0.01 ---- Precision: 0.822 / Recall: 0.682 / Accuracy: 0.758\n",
      "Est: 100 / Depth: 11 / LR: 0.1 ---- Precision: 0.836 / Recall: 0.814 / Accuracy: 0.82\n",
      "Est: 100 / Depth: 11 / LR: 1 ---- Precision: 0.811 / Recall: 0.805 / Accuracy: 0.801\n",
      "Est: 100 / Depth: 15 / LR: 0.01 ---- Precision: 0.809 / Recall: 0.699 / Accuracy: 0.758\n",
      "Est: 100 / Depth: 15 / LR: 0.1 ---- Precision: 0.837 / Recall: 0.813 / Accuracy: 0.82\n",
      "Est: 100 / Depth: 15 / LR: 1 ---- Precision: 0.802 / Recall: 0.795 / Accuracy: 0.791\n",
      "Est: 150 / Depth: 3 / LR: 0.01 ---- Precision: 0.828 / Recall: 0.547 / Accuracy: 0.705\n",
      "Est: 150 / Depth: 3 / LR: 0.1 ---- Precision: 0.847 / Recall: 0.752 / Accuracy: 0.8\n",
      "Est: 150 / Depth: 3 / LR: 1 ---- Precision: 0.815 / Recall: 0.803 / Accuracy: 0.802\n",
      "Est: 150 / Depth: 7 / LR: 0.01 ---- Precision: 0.816 / Recall: 0.64 / Accuracy: 0.738\n",
      "Est: 150 / Depth: 7 / LR: 0.1 ---- Precision: 0.833 / Recall: 0.806 / Accuracy: 0.815\n",
      "Est: 150 / Depth: 7 / LR: 1 ---- Precision: 0.82 / Recall: 0.806 / Accuracy: 0.807\n",
      "Est: 150 / Depth: 11 / LR: 0.01 ---- Precision: 0.82 / Recall: 0.701 / Accuracy: 0.764\n",
      "Est: 150 / Depth: 11 / LR: 0.1 ---- Precision: 0.83 / Recall: 0.814 / Accuracy: 0.817\n",
      "Est: 150 / Depth: 11 / LR: 1 ---- Precision: 0.81 / Recall: 0.79 / Accuracy: 0.794\n",
      "Est: 150 / Depth: 15 / LR: 0.01 ---- Precision: 0.815 / Recall: 0.718 / Accuracy: 0.768\n",
      "Est: 150 / Depth: 15 / LR: 0.1 ---- Precision: 0.842 / Recall: 0.818 / Accuracy: 0.825\n",
      "Est: 150 / Depth: 15 / LR: 1 ---- Precision: 0.801 / Recall: 0.8 / Accuracy: 0.792\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "def train_xgboost(n_est, max_depth, lr):\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=n_est, max_depth=max_depth, learning_rate=lr)\n",
    "    xgb_model.fit(X_train, y_train_encoded)\n",
    "    y_pred_encoded = xgb_model.predict(X_test)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='negative', average='binary')\n",
    "    accuracy = (y_pred == y_test).sum() / len(y_pred)\n",
    "    print('Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, max_depth, lr, round(precision, 3), round(recall, 3), round(accuracy, 3)))\n",
    "\n",
    "for n_est in [50, 100, 150]:\n",
    "    for max_depth in [3, 7, 11, 15]:\n",
    "        for lr in [0.01, 0.1, 1]:\n",
    "            train_xgboost(n_est, max_depth, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Results:\n",
      "Precision (Negative): 0.842\n",
      "Recall (Negative): 0.818\n",
      "Accuracy: 0.825\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "best_n_est = 150\n",
    "best_max_depth = 15\n",
    "best_lr = 0.1\n",
    "\n",
    "def train_final_xgboost():\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=best_n_est, max_depth=best_max_depth, learning_rate=best_lr)\n",
    "    xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "    y_pred_encoded = xgb_model.predict(X_test)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, pos_label='negative')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='negative')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Final Evaluation Results:\")\n",
    "    print(\"Precision (Negative):\", round(precision, 3))\n",
    "    print(\"Recall (Negative):\", round(recall, 3))\n",
    "    print(\"Accuracy:\", round(accuracy, 3))\n",
    "\n",
    "train_final_xgboost()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "For the training, the data was cleaned by removing the duplicates(102 entries). Then removed punctuation, performed Tokenization, Remove stopwords and done Stemming.\n",
    "\n",
    "In Random Forest Classifier model, the best overall performer was Est: 100 / Depth: None ----- Precision: 0.842 / Recall: 0.838 / Accuracy: 0.834\n",
    "\n",
    "Whereas for Gradient Boosting Classifier, Est: 150, Depth: 15, LR: 0.1 shows the best balance across all metrics with a precision of 0.842, recall of 0.818, and accuracy of 0.825. Therefore higher precision of 84.2% with fewer false positives. Higher recall means few false negatives. Accurancy shows that the model is correct 82.5% of the time.\n",
    "\n",
    "Comparing both the models, Random Forest Classifier model is preferable as it has higher recall and accurancy, precision remains same for both. Higher recall and accuracy suggest that the model is better at identifying negative cases and performs better overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
